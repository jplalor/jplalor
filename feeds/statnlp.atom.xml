<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>John Lalor</title><link href="http://jplalor.github.io/" rel="alternate"></link><link href="http://jplalor.github.io/feeds/statnlp.atom.xml" rel="self"></link><id>http://jplalor.github.io/</id><updated>2018-02-15T00:00:00-05:00</updated><entry><title>StatNLP: Essential Information Theory, Part 1</title><link href="http://jplalor.github.io/info-theory-part-1.html" rel="alternate"></link><updated>2018-02-15T00:00:00-05:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2018-02-15:info-theory-part-1.html</id><summary type="html">&lt;p&gt;I'd like to blog regularly, but I often struggle to come up with topics. So what I'm going to do is blog my way through some textbooks, to work on my writing and also to make sure that I have the concepts in the books down pat. The first textbook in the series will be &lt;em&gt;Foundations of Statistical Natural Language Processing&lt;/em&gt; by Manning and Sch&amp;uuml;tze (which I'll refer to as StatNLP from here on out). And the first topic will be &lt;em&gt;Essential Information Theory&lt;/em&gt; (section 2.2 in the book). This first post will introduce entropy and provide a few examples.&lt;/p&gt;
&lt;h1&gt;Information Theory Basics&lt;/h1&gt;
&lt;p&gt;Information Theory as a field was developed by Claude Shannon in the 1940s. I won't go into too many details, but basically Shannon was working on a way to quantify how much information one could transmit through a channel that would inevitably lose some of the information due to some amount of noise. He introduced some key concepts that we'll define and visualize here. Information Theory as a whole is an active field, and we'll only scratch the surface here, but at the end of the series I'll provide some pointers for anyone interested in learning more.&lt;/p&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;In this post we'll be looking at probabilities of random variables. We'll define a discrete random variable as a set of outcomes &lt;span class="math"&gt;\(X\)&lt;/span&gt;. Each potential outcome in &lt;span class="math"&gt;\(X\)&lt;/span&gt; has an associated probability: &lt;span class="math"&gt;\(p(X=x)\)&lt;/span&gt;. The &lt;em&gt;expectation&lt;/em&gt; (or &lt;em&gt;expected value&lt;/em&gt;) of &lt;span class="math"&gt;\(X\)&lt;/span&gt; is a weighted sum of the outcomes according to the probabilities:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{E}[X] = \sum_{x \in X} xp(x)$$&lt;/div&gt;
&lt;h2&gt;Entropy&lt;/h2&gt;
&lt;p&gt;One of the key pieces in Information Theory is entropy. Entropy is defined as the amount of &lt;em&gt;information&lt;/em&gt; in a random variable. The formula for entropy is &lt;/p&gt;
&lt;div class="math"&gt;$$H(X) = - \sum_{x \in X} p(x) \log p(x)$$&lt;/div&gt;
&lt;p&gt;Often times the logarithm in the formula is base 2, and the output is a measure of &lt;em&gt;bits&lt;/em&gt;. Sometimes you'll see the natural log (&lt;span class="math"&gt;\(\ln\)&lt;/span&gt;), and the output will be in &lt;em&gt;nats&lt;/em&gt;. You can also write this formula a few other ways. If you don't like the minus sign in the beginning there, you can move it in and use the reciprocal of the logarithm:&lt;/p&gt;
&lt;div class="math"&gt;$$H(X) = \sum_{x \in X} p(x) \log \frac{1}{p(x)}$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(\log \frac{1}{p(x)}\)&lt;/span&gt; is known as the Shannon information content of an outcome (or information content for short). It tells you how much information you get for a specific outcome of a random variable. The previous formula looks a lot like an expectation. That's because it is! So we can rewrite the formula for entropy again as:&lt;/p&gt;
&lt;div class="math"&gt;$$H(X) = \mathbb{E}( \log \frac{1}{p(X)})$$&lt;/div&gt;
&lt;p&gt;Notice in this last formula that the &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; is now &lt;span class="math"&gt;\(p(X)\)&lt;/span&gt; (capital &lt;span class="math"&gt;\(X\)&lt;/span&gt; instead of lowercase &lt;span class="math"&gt;\(x\)&lt;/span&gt;). When we have a lowercase &lt;span class="math"&gt;\(x\)&lt;/span&gt;, we're looking for the probability of a specific value &lt;span class="math"&gt;\(x\)&lt;/span&gt; which is in the set of possible values &lt;span class="math"&gt;\(X\)&lt;/span&gt;. In the third equation, we're calculating the expectation of the random variable &lt;span class="math"&gt;\(X\)&lt;/span&gt;, and so we use &lt;span class="math"&gt;\(p(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So what do all of these equivalent formulas mean? When we're calculating entropy, what we're really calculating is the amount of information contained in a particular random variable. On one extreme, if we know everything about the random variable (if &lt;span class="math"&gt;\(p(x) = 1\)&lt;/span&gt; for one of the potential outcomes), then there is no information to be gained from the random variable. On the other extreme, if all of the outcomes are equally likely (as in a standard six-sided die) then there is a lot of information in the random variable. &lt;/p&gt;
&lt;p&gt;I always like to think about entropy in terms of flipping a coin. Let's say we have a completely unbiased coin, so that the probability of it landing on heads is equal to the probability of it landing on tails. Our random variable is still &lt;span class="math"&gt;\(X\)&lt;/span&gt;, and the possible values are &lt;span class="math"&gt;\(x = H\)&lt;/span&gt; and &lt;span class="math"&gt;\(x = T\)&lt;/span&gt;. The probability of each outcome is &lt;span class="math"&gt;\(p(x = H) = p(x = T) = 0.5\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let's plug this in to our formula for entropy (we'll use the 2nd formula above).&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
H(X) &amp;amp;= \sum_{x \in \{H, T\}}p(x) \log \frac{1}{p(x)} \\
 &amp;amp;= 0.5 * \log \frac{1}{0.5} + 0.5 * \log \frac{1}{0.5} \\
 &amp;amp;= 2*(0.5 \log 2) \\
 &amp;amp;= 1
\end{align}
$$&lt;/div&gt;
&lt;p&gt;So what this tells us is that every time we flip an unbiased coin, we get 1 bit of information. what if the coin was biased? For example, let's say that the probability of heads is &lt;span class="math"&gt;\(0.75\)&lt;/span&gt; and the probability of tails is &lt;span class="math"&gt;\(0.25\)&lt;/span&gt;. In that case when we calculate entropy we get&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
H(X) &amp;amp;= \sum_{x \in \{H, T\}}p(x) \log \frac{1}{p(x)} \\
 &amp;amp;= 0.75 * \log \frac{1}{0.75} + 0.25 * \log \frac{1}{0.25} \\
 &amp;amp;\approx 0.81
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
We get less information if we flip a biased coin. Why is that? Because there is less uncertainty in the outcome of the coin flip. If the coin is biased, then one side is more likely to come up than the other. In the case above, if the probability of getting heads is &lt;span class="math"&gt;\(0.75\)&lt;/span&gt;, then before you even flip it the odds are good that it will come up heads. So you don't get as much information as when the coin is unbiased, where there is more uncertainty (in fact, the maximum amount of uncertainty).&lt;/p&gt;
&lt;p&gt;Let's look at a plot of entropy as the bias of a coin changes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="plot of chunk entropy_plot" src="http://jplalor.github.io/figure/info_theory-entropy_plot-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;As the plot shows, the more biased the coin is, the less information we gain when we flip it. Sometimes it helps to think about entropy in terms of being surprised. When we flip a coin biased to land on heads, we're less surprised when it does land on heads (lower entropy). But when a coin is unbiased, each flip is a surprise (high entropy).&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Entropy is a key building block for Information Theory, and it is also very useful in Machine Learning and NLP (more in a later post). Next time we'll look at entropy for more than one random variable, and define joint entropy, conditional entropy, and (probably) KL-Divergence.&lt;/p&gt;
&lt;p&gt;As a last note, I wrote this post in RMarkdown, which was surprisingly easy to integrate with this site, which runs on Pelican. All I needed was the &lt;a href="RMD Reader Pelican Plugin"&gt;https://github.com/getpelican/pelican-plugins/tree/master/rmd_reader&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="ml"></category><category term="nlp"></category></entry></feed>