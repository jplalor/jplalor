<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>John Lalor</title><link href="http://jplalor.github.io/" rel="alternate"></link><link href="http://jplalor.github.io/feeds/all.atom.xml" rel="self"></link><id>http://jplalor.github.io/</id><updated>2015-10-20T14:58:00-04:00</updated><entry><title>Dot Products and Vector Projections</title><link href="http://jplalor.github.io/vector-projections.html" rel="alternate"></link><updated>2015-10-20T14:58:00-04:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2015-10-20:vector-projections.html</id><summary type="html">&lt;p&gt;Every once in a while I'm going to write a post about something I learned in my Machine Learning course here at UMass. I'd like to do one post per lecture, but I'm late out of the gate, so it might be less frequent. These posts aren't intended to be thorough explanations of particular topics, but instead will be relatively high-level overviews, with a simple example to illustrate the concept. That's the plan anyway. Comments or corrections are welcome and encouraged.&lt;/p&gt;
&lt;p&gt;This post is about dot products and projections, mostly because they're really useful and came up early in lecture. The projection of a vector from one space to another can be very useful when trying to reduce the dimensionality of a set of vectors.&lt;/p&gt;
&lt;p&gt;Ok, quick pause for some background. What does that all mean?&lt;/p&gt;
&lt;p&gt;One way of thinking about data is as a collection of vectors. For example, if you are looking at a dataset where each entry is the height and weight of an individual, one of your vectors would look like this:&lt;/p&gt;
&lt;div class="math"&gt;$$v_a: \{70, 180\}$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(v_{a1}\)&lt;/span&gt; is individual a's height in inches and &lt;span class="math"&gt;\(v_{a2}\)&lt;/span&gt; is individual a's weight in pounds. This vector has a dimensionality of 2.&lt;/p&gt;
&lt;p&gt;Not all datasets will have dimensionality of 2. Many will have much higher dimensionality, which can make working with them difficult. One way of making life easier is attempting to take your high dimensional data and &lt;em&gt;project&lt;/em&gt; it onto a lower dimensional vector. In order to do this, we need to talk about dot products.&lt;/p&gt;
&lt;p&gt;Algebraically, the dot product of two vectors is the sum of the product of each pair of values in the vectors. So you take each pair of values from the two vectors, multiply them together, and add up all of those products.&lt;/p&gt;
&lt;div class="math"&gt;$$A\cdot B = \sum_{i=1}^n A_iB_i$$&lt;/div&gt;
&lt;p&gt;Knowing how to calculate a dot product, we can now project one vector onto another. In order to project vector B onto vector A, we take the dot product of B and A, and divide by the magnitude of A, which is really the square root of the dot product of A with itself, &lt;span class="math"&gt;\(\lvert\lvert A \rvert\rvert = \sqrt{A\cdot A}\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$comp_ab = \frac{A\cdot B}{\lvert\lvert A \rvert\rvert}$$&lt;/div&gt;
&lt;p&gt;This will give a scalar value, which is called the component or scalar projection of B in the A direction. You can think of it as the length of the projection of vector B. To turn this into a vector projection, we multiply it by the unit vector for A: &lt;span class="math"&gt;\(\frac{A}{\lvert\lvert A\rvert\rvert}\)&lt;/span&gt;. The formula for the projection is therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$proj_BA = \frac{A\cdot B}{\lvert\lvert A \rvert\rvert}\frac{A}{\lvert\lvert A\rvert\rvert}$$&lt;/div&gt;
&lt;p&gt;Let's look at an example to see how this all works. Consider two vectors:&lt;/p&gt;
&lt;div class="math"&gt;$$A = \{2, 4, 6\}$$&lt;/div&gt;
&lt;div class="math"&gt;$$B = \{10, 11, 12\}$$&lt;/div&gt;
&lt;p&gt;Let's project B onto A. First, we'll calculate the scalar projection:&lt;/p&gt;
&lt;div class="math"&gt;$$comp_AB = \frac{A\cdot B}{\lvert\lvert A\rvert\rvert}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ = \frac{2 * 10 + 4 * 11 + 6 * 12}{\sqrt{2 * 2 + 4 * 4 + 6 * 6}} = 18.174$$&lt;/div&gt;
&lt;p&gt;So our scalar projection is 18.174. Now, we take that and multiply it by the A unit vector:&lt;/p&gt;
&lt;div class="math"&gt;$$proj_AB = \frac{A\cdot B}{\lvert\lvert A \rvert\rvert}\frac{A}{\lvert\lvert A\rvert\rvert}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ = 18.174\frac{A}{\lvert\lvert A\rvert\rvert} = \{4.8571, 9.7143, 14.5714\}$$&lt;/div&gt;
&lt;p&gt;The result is the vector B &lt;em&gt;projected&lt;/em&gt; onto vector a.&lt;/p&gt;
&lt;p&gt;So there you have it. Dot products and projections. As you might have noticed, the vectors involved above were of the same length. But I thought we were going to project to lower dimensions? We will, we will. In order to do that we have to identify &lt;em&gt;basis vectors&lt;/em&gt; that identify the dimensions that we want to project to. That is a topic for a later post.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/dotprod/dotprod.html"&gt;http://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/dotprod/dotprod.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Vector_projection"&gt;https://en.wikipedia.org/wiki/Vector_projection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mathworld.wolfram.com/DotProduct.html"&gt;http://mathworld.wolfram.com/DotProduct.html&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="linalg"></category><category term="ml"></category></entry><entry><title>Hello!</title><link href="http://jplalor.github.io/hello.html" rel="alternate"></link><updated>2015-10-15T09:21:00-04:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2015-10-15:hello.html</id><summary type="html">&lt;p&gt;I've decided to start writing about what I've been learning as a new PhD student in Computer Science. I plan on writing about topics covered in classes I'm taking, things I come across during my research, and just general topics that I think are interesting.&lt;/p&gt;
&lt;p&gt;So we'll see how it goes.&lt;/p&gt;</summary></entry></feed>