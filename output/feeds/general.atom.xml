<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>John Lalor - general</title><link href="http://jplalor.github.io/" rel="alternate"></link><link href="http://jplalor.github.io/feeds/general.atom.xml" rel="self"></link><id>http://jplalor.github.io/</id><updated>2019-08-29T15:06:00-04:00</updated><entry><title>IRT with Artificial Crowds</title><link href="http://jplalor.github.io/emnlp19.html" rel="alternate"></link><published>2019-08-29T15:06:00-04:00</published><updated>2019-08-29T15:06:00-04:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2019-08-29:/emnlp19.html</id><summary type="html">&lt;p&gt;Because people don't want to label 50,000 sentences&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you have been keeping up with what's going on here my website, you'll know that a lot of my work involves Item Response Theory (IRT).
Typically, IRT requires human &lt;em&gt;response patterns&lt;/em&gt;, which means that each human has to label each item in your data set so that patterns across the individuals can be comapred when fitting the model.
This limits the size of the data sets that can be used, but even so we've been able to do some interesting work in NLP with IRT models using human data.&lt;/p&gt;
&lt;p&gt;This bottleneck leads to an obvious question: can we pull the humans out of this process and instead gather response patterns from an ensemble of models?
The models could act as our &lt;em&gt;artificial crowd&lt;/em&gt;, and the response patterns could be used to fit IRT models for very large data sets.&lt;/p&gt;
&lt;p&gt;To answer the question: you can, and we did.
The results are in our upcoming EMNLP 2019 paper: &lt;em&gt;Learning Latent Parameters without Human Response Patterns: Item Response Theory with Artificial Crowds&lt;/em&gt; (arXiv link coming soon).
This post will serve as a companion to the paper.&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;As I mentioned above, IRT can be really useful for NLP.
Information such as how difficult certain examples are, and how well a model performs with respect to a population (instead of just raw accuracy) can be used to better evaluate models, build better models, and sample training data.
But a major bottleneck in rolling IRT out has been the fact that IRT requires a lot of human data.
And it requires a lot of human data in two dimensions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For each example in your data set, you need a lot of labels for model-fitting (a few hundred or more)&lt;/li&gt;
&lt;li&gt;Each human has to label all (or at least most) of the examples in the data set&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first point means that data collection is expensive from a researcher's point of view.
The second point means that the data sets you can work with are somewhat limited in terms of size.
You could ask a human to label a few hundred or a thousand examples from a data set, but they would probably get bored (or angry) and the quality of those annotations would suffer.
So we wanted to see if we could fit IRT models with a crowd of models, since we can get a model to label a huge data set very cheaply.&lt;/p&gt;
&lt;h1&gt;Artificial Crowds&lt;/h1&gt;
&lt;p&gt;There are a number of ways you can construct an ensemble of models.
In this work we focused on &lt;em&gt;training data manipulation&lt;/em&gt; as the mechanism for building an artificial crowd.
For our two NLP data sets (SNLI and SSTB) we started with a baseline neural network model.
We used a straightforward LSTM model, nothing fancy.
With this baseline model, we ran 1000 training cycles, each time starting with a different subset of the training data set.
We did two manipulations to the training data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sample the training set size so that we have some subset of the training data&lt;/li&gt;
&lt;li&gt;flip the label for a randomly selected subset of this sample to introduce noise into training&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this setup each instance of the model is being trained with a very different data set, which will lead to very different results in terms of performance.
Once each of these models were trained we labeled all of the examples in the data set and used these outputs as the response patterns for fitting an IRT model.&lt;/p&gt;
&lt;h1&gt;Variational IRT&lt;/h1&gt;
&lt;p&gt;(Update) In looking back on these posts I realized that this section was just excluded. 
My apologies. 
In order to estimate our IRT parameters, we implemented a variational inference model proposed by Natesan et al. (2016). 
Each difficulty and ability parameter is assumed to be normally distributed and independent, and estimation involves minimizing the KL-divergence between the true posterior and the variational parameters. 
This has two key benefits:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It's Bayesian, so we get uncertainty estimates in the form of mean and variance parameters for each item.&lt;/li&gt;
&lt;li&gt;It's fast. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We implemented the variational IRT model in Pyro and used a hierarchical version that was shown to perform well in the prior work.&lt;/p&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;In order for all of this model training to be worth our while, there were three questions that we needed to answer:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Are the output parameters (reasonably) similar to those learned from a human crowd?&lt;/li&gt;
&lt;li&gt;Can we fit the IRT models for tens or hundreds of thousands of examples?&lt;/li&gt;
&lt;li&gt;If we can, who cares? (put another way: What downstream tasks can we use those learned parameters for?)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Comparing Humans and Machines&lt;/h2&gt;
&lt;p&gt;The first thing we did was have the artificial crowd label all of the examples that we already had human annotations for from our earlier work.
This way we can compare the learned parameters across the groups to see how well they correlate.&lt;/p&gt;
&lt;p&gt;For SNLI, the correlations look like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="correlations" src="figure/snli_scatter.png"&gt;&lt;/p&gt;
&lt;p&gt;And for SSTB:&lt;/p&gt;
&lt;p&gt;&lt;img alt="correlations_sstb" src="figure/sstb_scatter.png"&gt;&lt;/p&gt;
&lt;p&gt;So we can see that they are positive, which is nice.
Remember, they didn't have to be.
There is nothing that assumes that examples that are easy for humans are also easy for the models.
But it turns out for a lot of items that is the case, which is cool.&lt;/p&gt;
&lt;h2&gt;Putting it to Use&lt;/h2&gt;
&lt;p&gt;The next question then is, can we do this for very large data sets?
(If the answer was no then I probably wouldn't be writing this now would I?)
We compared fitting an IRT model with traditional methods to fitting one with variational inference, and saw that the results were very similar.
Once we scaled the variational inference method up to the response pattern data for all of SNLI and SSTB, we were able to fit IRT models in a few minutes for each one.&lt;/p&gt;
&lt;p&gt;Using the learned difficulty parameters, we conducted a data sampling experiment to see how sampling according to difficulty can impact model training.
Turns out that if you sample examples that are &lt;em&gt;average&lt;/em&gt; in terms of difficulty, you learn models that better generalize when the sampled data sets are very small:&lt;/p&gt;
&lt;p&gt;&lt;img alt="sampling" src="figure/thresholds.png"&gt;&lt;/p&gt;
&lt;p&gt;Sampling the easiest examples also does well, but by sampling average examples it seems like the model learns more generalizeable patterns than it does with just the easiest examples.&lt;/p&gt;
&lt;h2&gt;Where are the Differences?&lt;/h2&gt;
&lt;p&gt;Another interesting aspect of this work is the fact that we can compare easy and difficult examples across the human and machine populations.
A few patterns emerge, such as the need for external information and the presence of ambiguity, that cause a lot of the differences.
Tables showing the examples with the biggest differences, along with more discussion about possible causes, are in the paper.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;There are more results and technical details in the paper.
It's exciting to see that we can learn these IRT models with data collected from an artificial crowd.
This makes it much easier to fit an IRT model as part of a machine learning workflow.
You can see which specific examples are easiest/hardest for your task and use the information to inform model building and training.
Not to mention the fact that now we can use IRT to &lt;em&gt;evaluate&lt;/em&gt; models using very large machine learning data sets.
Hopefully this will encourage others to take a look at IRT and see how it can help in their machine learning and NLP research.&lt;/p&gt;
&lt;p&gt;Code for fitting your own IRT models is available &lt;a href="https://github.com/jplalor/py-irt"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Prathiba Natesan, Ratna Nandakumar, Tom Minka, and Jonathan D Rubright. 2016. Bayesian prior choice in irt estimation using mcmc and variational bayes. Frontiers in psychology, 7:1422.&lt;/p&gt;</content></entry><entry><title>A quick rant about language modeling.</title><link href="http://jplalor.github.io/openai.html" rel="alternate"></link><published>2019-02-19T14:56:00-05:00</published><updated>2019-02-19T14:56:00-05:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2019-02-19:/openai.html</id><summary type="html">&lt;p&gt;People need to have confidence in their peopleness&lt;/p&gt;</summary><content type="html">&lt;p&gt;(I never thought I would write a title like that)&lt;/p&gt;
&lt;p&gt;Everyone is going crazy over the OpenAI thing. &lt;/p&gt;
&lt;p&gt;You can go crazy if you like, that's fine with me, seems to be what all the cool kids are doing these days.
I'd point you to &lt;a href="http://approximatelycorrect.com/2019/02/17/openai-trains-language-model-mass-hysteria-ensues/"&gt;Zachary Lipton's post&lt;/a&gt; for how I generally feel about the research aspect of this.
The short version is: this is a better version of very standard natural language processing. 
That's it.
It's also not going to destroy us all, so rest easy.&lt;/p&gt;
&lt;p&gt;But all of that being said, this raises some interesting questions about AI policy.
Specifically, it feels like we're currently in the middle of a moment where a whole lot of people &lt;em&gt;want&lt;/em&gt; AI to take over.
That sounds very dramatic, so let me unpack it. &lt;/p&gt;
&lt;p&gt;Let's start at the very beginning (a very good place to start).
Recall, dear reader, the ELIZA program (&lt;a href="https://www.masswerk.at/eliza/"&gt;click for an online version&lt;/a&gt;).
ELIZA was an early chatbot built to mimic a very specific sort of psychotherapist.
There is a lot written about the history of ELIZA, and it is fascinating.
Some of the subjects who interacted with ELIZA formed an emotional attachment to the program.
If you interact with ELIZA now you can see very quickly that not only is it a program, but it is one with pretty severe limitations.&lt;/p&gt;
&lt;p&gt;Which brings us to the new announcement.
This week, OpenAI announced that they had built a language model that was &lt;em&gt;very good.&lt;/em&gt;
The language model is essentially a better version of stuff that already exists and has existed for a very long time (again, see Zachary's post).
But there is huge concern right now that if this particular bit of research were to get into the wrong hands, bad things would happen.
This is silly.
But let's indulge ourselves for a minute.&lt;/p&gt;
&lt;p&gt;Imagine that you are a person (shouldn't be too hard).
You are consuming &lt;strong&gt;news&lt;/strong&gt; via your favorite app/dead tree/whatever.
(The fact that I bolded news is important).
As you read through a story, you may or may not check the byline to determine who wrote the piece.
Knowing the author can signal quality or trustworthiness if the author is good or trustworthy.
If you don't know the author, you can at least trust the news source.
So, if you finished your article and thought to yourself, "that was really well-written" and then I jumped out from behind you and yelled "Aha! But it was written by an AI!" what would you do?
Other than kick me out of your house (justifiable)?
You would probably go back and consider the article more closely.
You may even say "Hmm, I don't know if I trust an AI to write news, I should fire up Google dot com and see if this is being reported elsewhere."
But it's unlikely that you will say "OK!" and be done with it.
Why? 
Because you are a human! 
You are not a news-ingesting machine that simply takes in news uncritically.&lt;/p&gt;
&lt;p&gt;Your new skeptical approach to news written by an AI should really be your default method for reading the news, but that is a post for another time.&lt;/p&gt;
&lt;p&gt;Now, new scenario.
Imagine that you are a person (I know, I'm really making you work here).
You are reading some collection of words that are &lt;strong&gt;not news.&lt;/strong&gt;
This could be a novel, a poem, or (importantly) a commentary written &lt;em&gt;about&lt;/em&gt; something in the news (I hope we're all on the same page that these commentaries aren't news).
If I jumped out after you finished reading this and told you that it was written by an AI, what would you do?
My argument here is that your answer should be "I don't know and I don't really care. I was entertained."
AI-generated entertainment shouldn't be cause for concern.&lt;/p&gt;
&lt;p&gt;If some form of artificial intelligence (in this case, a language model) can create entertainment via text to the point where you can read it and enjoy it, that is a remarkable accomplishment that should be celebrated.&lt;/p&gt;
&lt;p&gt;If some form of artificial intelligence can create textual descriptions of news events, that is an even more impressive feat.&lt;/p&gt;
&lt;p&gt;If the AI can write what looks like news but has factual errors (even if the structure is good), then the text isn't news.
We already have defenses against this kind of not-news: the ability to do research, find facts, and make informed decisions about things.&lt;/p&gt;
&lt;p&gt;What OpenAI is saying with their lack of transparency over this whole thing is that this new language model is dangerous.
But I would ask the following: if this model is so good that we don't trust humans to read its output, has OpenAI just passed the Turing test?
Doubtful.
There simply is a model that does a better job of matching patterns in writing as determined by a very large corpus of writing.&lt;/p&gt;
&lt;p&gt;So what should we do moving forward?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI should release the model, or change their name&lt;/li&gt;
&lt;li&gt;Humans should regain some confidence &lt;/li&gt;
&lt;li&gt;You should change the locks in your house so I can't jump out from behind you anymore&lt;/li&gt;
&lt;li&gt;You should spend less time reading stuff on the internet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The last point is most important.
Internet companies want you to spend time on the internet so that they can show you ads.
They have an incentive to provide &lt;em&gt;content&lt;/em&gt; in order to keep you on their pages.
They unfortunately do not currently have an incentive to provide you with &lt;em&gt;facts.&lt;/em&gt;
The more time you spend on the internet, the more content you see, the more ads you see, and the more money these companies get.
So, if they need more and more content, and fancy AI systems are writing top notch content now, then they will use said systems to reduce the cost of content.&lt;/p&gt;
&lt;p&gt;It's not clear if there will be an increased cost associated with the potentially-less-than-totally-true nature of the AI content.
But looking at fake news now, there isn't much cost associated with putting fake news online, so I don't see there being big costs to putting AI-fake-news online either.&lt;/p&gt;
&lt;p&gt;So if you want to prevent AI from taking over the world of online publishing, then don't read as much entertainment online.
Or if you are reading something online and you have your doubts as to whether it is true or not, do some digging! Find out for yourself.&lt;/p&gt;</content></entry></feed>