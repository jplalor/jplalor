<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>John Lalor</title><link href="http://jplalor.github.io/" rel="alternate"></link><link href="http://jplalor.github.io/feeds/all.atom.xml" rel="self"></link><id>http://jplalor.github.io/</id><updated>2018-03-28T00:00:00-04:00</updated><entry><title>StatNLP: Essential Information Theory, Part 3</title><link href="http://jplalor.github.io/info-theory-part-3.html" rel="alternate"></link><published>2018-03-28T00:00:00-04:00</published><updated>2018-03-28T00:00:00-04:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2018-03-28:/info-theory-part-3.html</id><summary type="html">&lt;p&gt;Wrapping up the Information Theory Introduction&lt;/p&gt;</summary><content type="html">&lt;p&gt;After a bit of a hiatus, it's time to wrap up the Information Theory introduction. In this post I'm going to talk about Mutual Information and KL-Divergence. I'll look at why these two are important, how we get them from what we already know, and what we can do with them (with an example or two for good measure). After this post we can hit the ground running with the StatNLP book, woohoo!&lt;/p&gt;
&lt;h2&gt;Mutual Information&lt;/h2&gt;
&lt;p&gt;At the end of the previous post, we looked at the relationship between joint entropy and conditional entropy. Recall that &lt;span class="math"&gt;\(H(X,Y) = H(X) + H(Y \vert X)\)&lt;/span&gt;. In this formula, we can reverse &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y\)&lt;/span&gt; to get &lt;span class="math"&gt;\(H(X,Y) = H(Y) + H(X \vert Y)\)&lt;/span&gt;. The two are equivalent. If we rearrange those formulas we can derive the formula for &lt;em&gt;mutual information:&lt;/em&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
H(X) + H(Y \vert X) &amp;amp;= H(Y) + H(X \vert Y) \\
H(X) - H(X \vert Y) &amp;amp;= H(Y) - H(Y \vert X) \\
&amp;amp;= I(X;Y)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
Mutual information describes the reduction in uncertainty of one random variable that you get if you know another random variable. For example if &lt;span class="math"&gt;\(X\)&lt;/span&gt; is a random variable indicating whether it is currently raining or not, and &lt;span class="math"&gt;\(Y\)&lt;/span&gt; is a random variable indicating whether the ground or not is wet, if you know that the ground is wet (&lt;span class="math"&gt;\(Y=1\)&lt;/span&gt;), that will reduce the amount of uncertainty in determining whether it is raining (&lt;span class="math"&gt;\(X\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;How do you calculate mutual information? From Equation 2.36 in StatNLP:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
I(X;Y) &amp;amp;= H(X) - H(X \vert Y) \\
&amp;amp;= H(X) + H(Y) - H(X,Y)  \textit{ by the chain rule} \\
&amp;amp;= \sum_x p(x) \log \frac{1}{p(x)} + \sum_y p(y) \log \frac{1}{p(y)} + \sum_{x,y} p(x,y) \log p(x,y) \\
&amp;amp;= \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;In the case where two random variables are independent, then &lt;span class="math"&gt;\(I(X;Y) = 0\)&lt;/span&gt;. Why? Because &lt;span class="math"&gt;\(H(X \vert Y) = H(X)\)&lt;/span&gt;, so we have &lt;span class="math"&gt;\(I(X;Y) = H(X) - H(X) = 0\)&lt;/span&gt;. This makes sense since mutual information is measuring the reduction in uncertainty for one variable when you know another one. If you know &lt;span class="math"&gt;\(Y\)&lt;/span&gt;, but it is independent of &lt;span class="math"&gt;\(X\)&lt;/span&gt;, there is no reduction in uncertainty for &lt;span class="math"&gt;\(X\)&lt;/span&gt;. &lt;/p&gt;
&lt;h2&gt;KL-Divergence&lt;/h2&gt;
&lt;p&gt;The next definition is for Kullback-Leibler divergence, or KL-divergence. KL-divergence measures how far one probability distribution is from another:&lt;/p&gt;
&lt;div class="math"&gt;$$
D(p \vert \vert q) = \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}
$$&lt;/div&gt;
&lt;p&gt;
It's worth noting that KL-divergence is not a distance metric since it is not symmetric, &lt;span class="math"&gt;\(D(p \vert \vert q) \neq D(q \vert \vert p)\)&lt;/span&gt;. It's useful for determining how good of an estimate q is for p. And looking at our equation for mutual information, we see that it can be expressed as a KL-divergence: &lt;span class="math"&gt;\(I(X;Y) = D(p(x,y) \vert \vert p(x)p(y))\)&lt;/span&gt;. This shows us that mutual information is a way to quantify the dependence of a joint distribution. &lt;/p&gt;
&lt;h2&gt;Cross Entropy&lt;/h2&gt;
&lt;p&gt;Imagine that you have some random variable &lt;span class="math"&gt;\(X\)&lt;/span&gt; with some true probability distribution &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;. You don't know what &lt;span class="math"&gt;\(p\)&lt;/span&gt; is, but you have this other distribution &lt;span class="math"&gt;\(q\)&lt;/span&gt;, and you want to know how good of a job it does at approximating &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;. To answer this question requires &lt;em&gt;cross entropy:&lt;/em&gt; &lt;span class="math"&gt;\(H(X, q) = H(X) + D(p \vert \vert q)\)&lt;/span&gt;. Now wait a minute. The formula for cross entropy still requires that we know &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;. Let's rearrange some terms:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
H(X, q) &amp;amp;= H(X) + D(p \vert \vert q) \\
&amp;amp;= - \sum_x p(x) \log p(x) + \sum_x p(x) \log \frac{p(x)}{q(x)} \\
&amp;amp;= - \sum_x p(x) \log p(x) + \sum_x p(x) \log p(x) + \sum_x p(x) \log \frac{1}{q(x)} \\
&amp;amp;= \sum_x p(x) \frac{1}{\log q(x)} \\
&amp;amp;= \mathbb{E}_p [\log \frac{1}{q(x)}]
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
We can calculate this if we know &lt;span class="math"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;These last few posts had a lot of definitions and math in them, but hopefully they are helpful for defining some key concepts in Information Theory. Entropy is a useful tool for machine learning and NLP, and upcoming posts will be using these concepts.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="ml"></category><category term="nlp"></category></entry><entry><title>StatNLP: Essential Information Theory, Part 2</title><link href="http://jplalor.github.io/info-theory-part-2.html" rel="alternate"></link><published>2018-02-23T00:00:00-05:00</published><updated>2018-02-23T00:00:00-05:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2018-02-23:/info-theory-part-2.html</id><summary type="html">&lt;p&gt;Joint and Conditional Entropy&lt;/p&gt;</summary><content type="html">&lt;p&gt;Welcome back! In the last post we started our discussion of Information Theory, and defined some key building blocks related to calculating information for a single random variable. This time we'll look at the relationships between two random variables, and what they can tell us (if anything) about the informativeness of each other.&lt;/p&gt;
&lt;h2&gt;Joint Entropy&lt;/h2&gt;
&lt;p&gt;The first thing we'll look at is &lt;strong&gt;joint entropy&lt;/strong&gt;. For two random variables, &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y\)&lt;/span&gt;, the joint entropy between them is &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
H(X, Y) &amp;amp;= - \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y) \\
&amp;amp;= \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{p(x, y)}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;Joint entropy refers to the amount of information you would need to determine the values of two random variables.
As an example, we'll go back to coin flipping from last time.
Now, we have two independent coins, both of them unbiased (so &lt;span class="math"&gt;\(p(x=H) = p(x=T) = p(y=H) = p(y=T) = 0.5\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The joint entropy for these two random variables is as follows (below the order for the variables is &lt;span class="math"&gt;\((x,y)\)&lt;/span&gt;, so &lt;span class="math"&gt;\(p(H,H)\)&lt;/span&gt; is really &lt;span class="math"&gt;\(p(x=H, y=H)\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
H(X, Y) &amp;amp;=  \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{ p(x, y)} \\
&amp;amp;= p(H,H)\log \frac{1}{p(H,H)} + p(H,T)\log \frac{1}{p(H,T)} \\
&amp;amp;+ p(T,H)\log \frac{1}{p(T,H)} + p(T,T)\log \frac{1}{p(T,T)} \\
&amp;amp;= \frac{1}{4}\log 4 + \frac{1}{4}\log 4+ \frac{1}{4}\log 4+ \frac{1}{4}\log 4 \\
&amp;amp;= 2
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
This should make sense, since we showed last time that the entropy of a single unbiased coin is 1 bit. If we have two coins, we then double our information.&lt;/p&gt;
&lt;h2&gt;Conditional Entropy&lt;/h2&gt;
&lt;p&gt;Conditional entropy is related to joint entropy. We're still dealing with two random variables, but with conditional entropy we're interested in how much information is gained &lt;em&gt;when we know one of the outcomes.&lt;/em&gt; &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
H(Y \vert X) &amp;amp;= \sum_{x \in X} p(x) H(Y \vert X = x) \\
&amp;amp;= \sum_{x \in X} p(x) [\sum_{y \in Y} p(y \vert x) \log \frac{1}{p(y \vert x)}] \\
&amp;amp;= \sum_{x \in X} \sum_{y \in Y} p(x, y) log \frac{1}{p(y \vert x)}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;Ok, so what just happened? Let's take the previous set of equations one at a time. In the first line we define the conditional entropy as a weighted average of the conditional entropies of &lt;span class="math"&gt;\(Y\)&lt;/span&gt; given each possible value of &lt;span class="math"&gt;\(X\)&lt;/span&gt;. In the second line we just expand that out, using the definition of entropy. Finally, &lt;span class="math"&gt;\(p(x)p(y|x) = p(x,y)\)&lt;/span&gt; so we can rewrite the equation as in the third line.&lt;/p&gt;
&lt;p&gt;Joint and conditional entropy are closely related. With joint entropy, we are measuring the amount of information in two random variables. Conditional entropy we already know one of the two random variables, and so we're measuring how much additional information is in the other. We can also look at it mathematically, starting with the definition of joint entropy:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
H(X, Y) &amp;amp;=  \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{ p(x, y)} \\
&amp;amp;= \sum_{x \in X} \sum_{y \in Y} p(x,y) \log \frac{1}{ p(x) p(y\vert x)} \\
&amp;amp;= \sum_{x \in X} \sum_{y \in Y} p(x,y)[ \log \frac{1}{p(x)} + \log \frac{1}{p(y \vert x)}] \\
&amp;amp;= \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{p(x)} + \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{p(y \vert x)} \\
&amp;amp;= \sum_{x \in X} p(x) \log \frac{1}{p(x)} + \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{p(y \vert x)} \\
&amp;amp;= H(X) + H(Y \vert X)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;Let's walk through that. In eqn. 12, we rewrite &lt;span class="math"&gt;\(p(x,y)\)&lt;/span&gt; in the log. In eqn 13. we split that log product into a sum of logs. In eqn. 14 we move &lt;span class="math"&gt;\(p(x,y)\)&lt;/span&gt; into the sum, and in eqn. 15 we use the fact that &lt;span class="math"&gt;\(\sum_{y \in Y} p(x, y) = p(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So to get the joint entropy of two random variables, you calculate the entropy of one, and add the conditional entropy of the other, given the first one. If the two random variables are independent, then &lt;span class="math"&gt;\(H(X, Y) = H(X) + H(Y)\)&lt;/span&gt;. We saw that above with our two coin example. If you swap all of the x's and y's in the above, you'll see that the order doesn't matter, and &lt;span class="math"&gt;\(H(X,Y) = H(X) + H(Y \vert X) = H(Y) + H(X \vert Y)\)&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;There were a lot of equations here, but hopefully they all make sense. We're still trying to measure information (as in standard entropy for a single random variable) but now we can look at the information relationship between pairs of random variables. Next time we'll look at two more advanced concepts: mutual information and KL-Divergence.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="ml"></category><category term="nlp"></category></entry><entry><title>StatNLP: Essential Information Theory, Part 1</title><link href="http://jplalor.github.io/info-theory-part-1.html" rel="alternate"></link><published>2018-02-15T00:00:00-05:00</published><updated>2018-02-15T00:00:00-05:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2018-02-15:/info-theory-part-1.html</id><summary type="html">&lt;p&gt;Entropy!&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'd like to blog regularly, but I often struggle to come up with topics. So what I'm going to do is blog my way through some textbooks, to work on my writing and also to make sure that I have the concepts in the books down pat. The first textbook in the series will be &lt;em&gt;Foundations of Statistical Natural Language Processing&lt;/em&gt; by Manning and Sch&amp;uuml;tze (which I'll refer to as StatNLP from here on out). And the first topic will be &lt;em&gt;Essential Information Theory&lt;/em&gt; (section 2.2 in the book). This first post will introduce entropy and provide a few examples.&lt;/p&gt;
&lt;h1&gt;Information Theory Basics&lt;/h1&gt;
&lt;p&gt;Information Theory as a field was developed by Claude Shannon in the 1940s. I won't go into too many details, but basically Shannon was working on a way to quantify how much information one could transmit through a channel that would inevitably lose some of the information due to some amount of noise. He introduced some key concepts that we'll define and visualize here. Information Theory as a whole is an active field, and we'll only scratch the surface here, but at the end of the series I'll provide some pointers for anyone interested in learning more.&lt;/p&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;In this post we'll be looking at probabilities of random variables. We'll define a discrete random variable as a set of outcomes &lt;span class="math"&gt;\(X\)&lt;/span&gt;. Each potential outcome in &lt;span class="math"&gt;\(X\)&lt;/span&gt; has an associated probability: &lt;span class="math"&gt;\(p(X=x)\)&lt;/span&gt;. The &lt;em&gt;expectation&lt;/em&gt; (or &lt;em&gt;expected value&lt;/em&gt;) of &lt;span class="math"&gt;\(X\)&lt;/span&gt; is a weighted sum of the outcomes according to the probabilities:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbb{E}[X] = \sum_{x \in X} xp(x)$$&lt;/div&gt;
&lt;h2&gt;Entropy&lt;/h2&gt;
&lt;p&gt;One of the key pieces in Information Theory is entropy. Entropy is defined as the amount of &lt;em&gt;information&lt;/em&gt; in a random variable. The formula for entropy is &lt;/p&gt;
&lt;div class="math"&gt;$$H(X) = - \sum_{x \in X} p(x) \log p(x)$$&lt;/div&gt;
&lt;p&gt;Often times the logarithm in the formula is base 2, and the output is a measure of &lt;em&gt;bits&lt;/em&gt;. Sometimes you'll see the natural log (&lt;span class="math"&gt;\(\ln\)&lt;/span&gt;), and the output will be in &lt;em&gt;nats&lt;/em&gt;. You can also write this formula a few other ways. If you don't like the minus sign in the beginning there, you can move it in and use the reciprocal of the logarithm:&lt;/p&gt;
&lt;div class="math"&gt;$$H(X) = \sum_{x \in X} p(x) \log \frac{1}{p(x)}$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(\log \frac{1}{p(x)}\)&lt;/span&gt; is known as the Shannon information content of an outcome (or information content for short). It tells you how much information you get for a specific outcome of a random variable. The previous formula looks a lot like an expectation. That's because it is! So we can rewrite the formula for entropy again as:&lt;/p&gt;
&lt;div class="math"&gt;$$H(X) = \mathbb{E}( \log \frac{1}{p(X)})$$&lt;/div&gt;
&lt;p&gt;Notice in this last formula that the &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; is now &lt;span class="math"&gt;\(p(X)\)&lt;/span&gt; (capital &lt;span class="math"&gt;\(X\)&lt;/span&gt; instead of lowercase &lt;span class="math"&gt;\(x\)&lt;/span&gt;). When we have a lowercase &lt;span class="math"&gt;\(x\)&lt;/span&gt;, we're looking for the probability of a specific value &lt;span class="math"&gt;\(x\)&lt;/span&gt; which is in the set of possible values &lt;span class="math"&gt;\(X\)&lt;/span&gt;. In the third equation, we're calculating the expectation of the random variable &lt;span class="math"&gt;\(X\)&lt;/span&gt;, and so we use &lt;span class="math"&gt;\(p(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So what do all of these equivalent formulas mean? When we're calculating entropy, what we're really calculating is the amount of information contained in a particular random variable. On one extreme, if we know everything about the random variable (if &lt;span class="math"&gt;\(p(x) = 1\)&lt;/span&gt; for one of the potential outcomes), then there is no information to be gained from the random variable. On the other extreme, if all of the outcomes are equally likely (as in a standard six-sided die) then there is a lot of information in the random variable. &lt;/p&gt;
&lt;p&gt;I always like to think about entropy in terms of flipping a coin. Let's say we have a completely unbiased coin, so that the probability of it landing on heads is equal to the probability of it landing on tails. Our random variable is still &lt;span class="math"&gt;\(X\)&lt;/span&gt;, and the possible values are &lt;span class="math"&gt;\(x = H\)&lt;/span&gt; and &lt;span class="math"&gt;\(x = T\)&lt;/span&gt;. The probability of each outcome is &lt;span class="math"&gt;\(p(x = H) = p(x = T) = 0.5\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let's plug this in to our formula for entropy (we'll use the 2nd formula above).&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
H(X) &amp;amp;= \sum_{x \in \{H, T\}}p(x) \log \frac{1}{p(x)} \\
 &amp;amp;= 0.5 * \log \frac{1}{0.5} + 0.5 * \log \frac{1}{0.5} \\
 &amp;amp;= 2*(0.5 \log 2) \\
 &amp;amp;= 1
\end{align}
$$&lt;/div&gt;
&lt;p&gt;So what this tells us is that every time we flip an unbiased coin, we get 1 bit of information. what if the coin was biased? For example, let's say that the probability of heads is &lt;span class="math"&gt;\(0.75\)&lt;/span&gt; and the probability of tails is &lt;span class="math"&gt;\(0.25\)&lt;/span&gt;. In that case when we calculate entropy we get&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
H(X) &amp;amp;= \sum_{x \in \{H, T\}}p(x) \log \frac{1}{p(x)} \\
 &amp;amp;= 0.75 * \log \frac{1}{0.75} + 0.25 * \log \frac{1}{0.25} \\
 &amp;amp;\approx 0.81
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
We get less information if we flip a biased coin. Why is that? Because there is less uncertainty in the outcome of the coin flip. If the coin is biased, then one side is more likely to come up than the other. In the case above, if the probability of getting heads is &lt;span class="math"&gt;\(0.75\)&lt;/span&gt;, then before you even flip it the odds are good that it will come up heads. So you don't get as much information as when the coin is unbiased, where there is more uncertainty (in fact, the maximum amount of uncertainty).&lt;/p&gt;
&lt;p&gt;Let's look at a plot of entropy as the bias of a coin changes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="plot of chunk entropy_plot" src="http://jplalor.github.io/figure/info_theory-entropy_plot-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;As the plot shows, the more biased the coin is, the less information we gain when we flip it. Sometimes it helps to think about entropy in terms of being surprised. When we flip a coin biased to land on heads, we're less surprised when it does land on heads (lower entropy). But when a coin is unbiased, each flip is a surprise (high entropy).&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Entropy is a key building block for Information Theory, and it is also very useful in Machine Learning and NLP (more in a later post). Next time we'll look at entropy for more than one random variable, and define joint entropy, conditional entropy, and (probably) KL-Divergence.&lt;/p&gt;
&lt;p&gt;As a last note, I wrote this post in RMarkdown, which was surprisingly easy to integrate with this site, which runs on Pelican. All I needed was the &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/rmd_reader"&gt;RMD Reader Pelican Plugin&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="ml"></category><category term="nlp"></category></entry><entry><title>Confession: I'm a Nodder</title><link href="http://jplalor.github.io/confession-im-a-nodder.html" rel="alternate"></link><published>2016-12-08T13:11:00-05:00</published><updated>2016-12-08T13:11:00-05:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2016-12-08:/confession-im-a-nodder.html</id><summary type="html">&lt;p&gt;How I act when I'm listening to a talk...&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was preparing my slides for my recent MLFL talk, and I came across this great page on &lt;a href="https://www.microsoft.com/en-us/research/academic-program/give-great-research-talk/"&gt;How to give a great presentation&lt;/a&gt; by Simon Peyton Jones at MSR.
First off, it was a huge help in getting ready for the presentation, I was still very nervous but the tips in the slides were a big help.&lt;/p&gt;
&lt;p&gt;One thing that struck me was the slide on Being Heard.
Specifically, trying to identify a &lt;em&gt;nodder&lt;/em&gt; and speaking to him or her.&lt;/p&gt;
&lt;p&gt;I never thought about it before, but I am definitely a nodder when I attend talks. 
My head is constantly bobbing as I listen to speakers present their work.
So keep an eye out for me if I'm attending your talk, I'll be nodding away!&lt;/p&gt;</content></entry><entry><title>Dot Products and Vector Projections</title><link href="http://jplalor.github.io/vector-projections.html" rel="alternate"></link><published>2015-10-20T14:58:00-04:00</published><updated>2015-10-20T14:58:00-04:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2015-10-20:/vector-projections.html</id><summary type="html">&lt;p&gt;Because this stuff is useful&lt;/p&gt;</summary><content type="html">&lt;p&gt;Every once in a while I'm going to write a post about something I learned in my Machine Learning course here at UMass. I'd like to do one post per lecture, but I'm late out of the gate, so it might be less frequent. These posts aren't intended to be thorough explanations of particular topics, but instead will be relatively high-level overviews, with a simple example to illustrate the concept. That's the plan anyway. Comments or corrections are welcome and encouraged.&lt;/p&gt;
&lt;p&gt;This post is about dot products and projections, mostly because they're really useful and came up early in lecture. The projection of a vector from one space to another can be very useful when trying to reduce the dimensionality of a set of vectors.&lt;/p&gt;
&lt;p&gt;Ok, quick pause for some background. What does that all mean?&lt;/p&gt;
&lt;p&gt;One way of thinking about data is as a collection of vectors. For example, if you are looking at a dataset where each entry is the height and weight of an individual, one of your vectors would look like this:&lt;/p&gt;
&lt;div class="math"&gt;$$v_a: \{70, 180\}$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(v_{a1}\)&lt;/span&gt; is individual a's height in inches and &lt;span class="math"&gt;\(v_{a2}\)&lt;/span&gt; is individual a's weight in pounds. This vector has a dimensionality of 2.&lt;/p&gt;
&lt;p&gt;Not all datasets will have dimensionality of 2. Many will have much higher dimensionality, which can make working with them difficult. One way of making life easier is attempting to take your high dimensional data and &lt;em&gt;project&lt;/em&gt; it onto a lower dimensional vector. In order to do this, we need to talk about dot products.&lt;/p&gt;
&lt;p&gt;Algebraically, the dot product of two vectors is the sum of the product of each pair of values in the vectors. So you take each pair of values from the two vectors, multiply them together, and add up all of those products.&lt;/p&gt;
&lt;div class="math"&gt;$$A\cdot B = \sum_{i=1}^n A_iB_i$$&lt;/div&gt;
&lt;p&gt;Knowing how to calculate a dot product, we can now project one vector onto another. In order to project vector B onto vector A, we take the dot product of B and A, and divide by the magnitude of A, which is really the square root of the dot product of A with itself, &lt;span class="math"&gt;\(\lvert\lvert A \rvert\rvert = \sqrt{A\cdot A}\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$comp_ab = \frac{A\cdot B}{\lvert\lvert A \rvert\rvert}$$&lt;/div&gt;
&lt;p&gt;This will give a scalar value, which is called the component or scalar projection of B in the A direction. You can think of it as the length of the projection of vector B. To turn this into a vector projection, we multiply it by the unit vector for A: &lt;span class="math"&gt;\(\frac{A}{\lvert\lvert A\rvert\rvert}\)&lt;/span&gt;. The formula for the projection is therefore:&lt;/p&gt;
&lt;div class="math"&gt;$$proj_BA = \frac{A\cdot B}{\lvert\lvert A \rvert\rvert}\frac{A}{\lvert\lvert A\rvert\rvert}$$&lt;/div&gt;
&lt;p&gt;Let's look at an example to see how this all works. Consider two vectors:&lt;/p&gt;
&lt;div class="math"&gt;$$A = \{2, 4, 6\}$$&lt;/div&gt;
&lt;div class="math"&gt;$$B = \{10, 11, 12\}$$&lt;/div&gt;
&lt;p&gt;Let's project B onto A. First, we'll calculate the scalar projection:&lt;/p&gt;
&lt;div class="math"&gt;$$comp_AB = \frac{A\cdot B}{\lvert\lvert A\rvert\rvert}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ = \frac{2 * 10 + 4 * 11 + 6 * 12}{\sqrt{2 * 2 + 4 * 4 + 6 * 6}} = 18.174$$&lt;/div&gt;
&lt;p&gt;So our scalar projection is 18.174. Now, we take that and multiply it by the A unit vector:&lt;/p&gt;
&lt;div class="math"&gt;$$proj_AB = \frac{A\cdot B}{\lvert\lvert A \rvert\rvert}\frac{A}{\lvert\lvert A\rvert\rvert}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ = 18.174\frac{A}{\lvert\lvert A\rvert\rvert} = \{4.8571, 9.7143, 14.5714\}$$&lt;/div&gt;
&lt;p&gt;The result is the vector B &lt;em&gt;projected&lt;/em&gt; onto vector a.&lt;/p&gt;
&lt;p&gt;So there you have it. Dot products and projections. As you might have noticed, the vectors involved above were of the same length. But I thought we were going to project to lower dimensions? We will, we will. In order to do that we have to identify &lt;em&gt;basis vectors&lt;/em&gt; that identify the dimensions that we want to project to. That is a topic for a later post.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/dotprod/dotprod.html"&gt;http://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/dotprod/dotprod.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Vector_projection"&gt;https://en.wikipedia.org/wiki/Vector_projection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mathworld.wolfram.com/DotProduct.html"&gt;http://mathworld.wolfram.com/DotProduct.html&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="linalg"></category><category term="ml"></category></entry><entry><title>Hello!</title><link href="http://jplalor.github.io/hello.html" rel="alternate"></link><published>2015-10-15T09:21:00-04:00</published><updated>2015-10-15T09:21:00-04:00</updated><author><name>John Lalor</name></author><id>tag:jplalor.github.io,2015-10-15:/hello.html</id><summary type="html">&lt;p&gt;I'm starting a blog...&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've decided to start writing about what I've been learning as a new PhD student in Computer Science. I plan on writing about topics covered in classes I'm taking, things I come across during my research, and just general topics that I think are interesting.&lt;/p&gt;
&lt;p&gt;So we'll see how it goes.&lt;/p&gt;</content></entry></feed>