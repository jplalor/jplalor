<!DOCTYPE html>
<html lang="en">

  <head>
      <title>John Lalor</title>
      <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,700italic,400,600,700' rel='stylesheet' type='text/css' />
      <link href='http://fonts.googleapis.com/css?family=Merriweather:300' rel='stylesheet' type='text/css'/>
      <link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:200,400,700' rel='stylesheet' type='text/css'/>
      <link rel="stylesheet" type="text/css" href="../theme/css/icons.css"/>
      <link rel="stylesheet" type="text/css" href="../theme/css/styles.css"/>
      <meta charset="utf-8" />
  </head>

  <body id="index">
    <!-- header -->
    <header class="siteheader">
      <!-- site image -->
        <div class= "siteimage">
          <a class="nodec" href=images/me_head_2.png>
            <img width="200" height="200" src=images/me_head_2.png>
          </a>
        </div>

      <div class = "sitebanner">
        <h1><a class="sitetitle nodec" href="..">John Lalor</a></h1>
        <h3 class ="sitesubtitle"></h3>
        <!-- nav -->
        <nav class="menu">
          <ul>
            <!-- menu items-->
              <li><a class="nodec" href="/">home</a></li>
              <li><a class="nodec" href="/blog_index.html">blog</a></li>
              <li><a class="nodec" href="/pdfs/cv.pdf">cv</a></li>
            <!--pages-->
            <!-- services icons -->
              <li><a class="nodec icon-mail-alt" href="mailto:lalor@cs.umass.edu"></a></li>
              <li><a class="nodec icon-github" href="https://github.com/jplalor"></a></li>
          </ul>
        </nav>
      </div> <!-- sitebanner -->
    </header>

    <!-- content -->

<section class="content">

  <h3 class="posttitle">
    <a class="nodec" href="/drafts/openai.html" rel="bookmark" title="Permalink to AI is not going to destroy us all">
      AI is not going to destroy us all
    </a>
  </h3>

  <div class="postinfo">
    <p class="published" title="2019-02-18T10:00:00-05:00">
      Mon 18 February 2019
    </p>

  </div><!-- .postinfo -->

  <div class="article">
    <p>Everyone is going crazy over the OpenAI thing. 
You can go crazy if you like, that's fine with me, seems to be what all the cool kids are doing these days.
I'd point you to Zach Lipton's post for how I generally feel about the whole thing.
The short version is: this is a better version of very standard natural language processing. 
That's it.
It's also not going to destroy us all, so rest easy.</p>
<p>But all of that being said, I've been thinking a lot recently about AI, machine learning, and people.
That kinda covers a lot, I know.
But specifically, it feels like we're currently in the middle of a moment where a whole lot of people <em>want</em> AI to take over.
That sounds very dramatic, so let me unpack it. </p>
<p>I want to make 2 points through this post:</p>
<ol>
<li>We humans are selling ourselves short</li>
<li>If you consider the most sensational aspect of any type of research as a prerogative to shut said research down, you won't get anywhere.</li>
</ol>
<p>Let's start at the very beginning (a very good place to start).
Recall, dear reader, the ELIZA program.
ELIZA was an early chatbot built to mimic a very specific sort of psychotherapist.
It wasn't very good, but at the time it was the first chatbot and people went wild over it.
Anecdotally, people spent too much time with it, thought it was human, and generally overreacted to what in essence was a computerized repeater-thingy.</p>
<p>This week, OpenAI announced that they had built a language model that was <em>very good.</em>
The language model is essentially a better version of stuff that already exists and has existed for a very long time (again, see Zach's post).
But there is huge concern right now that if this particular bit of research were to get into the wrong hands, bad things would happen.
This is silly.
But let's indulge ourselves for a minute.</p>
<p>Imagine that you are a person (shouldn't be too hard).
You are consuming <strong>news</strong> via your favorite app/dead tree/whatever.
(The fact that I bolded news is important).
As you read through a story, you may or may not check the byline to determine who wrote the piece.
Knowing the author can signal quality or trustworthiness if the author is good or trustworthy.
If you don't know the author, you can at least trust the news source.
So, if you finished your article and thought to yourself, "that was really well-written" and then I jumped out from behind you and yelled "Aha! But it was written by an AI!" what would you do?
Other than kick me out of your house (justifiable).
You would probably go back and consider the article more closely.
You may even say "Hmm, I don't know if I trust an AI to write news, I should fire up Google dot com and see if this is being reported elsewhere."
But it's unlikely that you will say "OK!" and be done with it.
Why? 
Because you are a human! 
You are not a news-injesting machine that simply takes in news uncritically.</p>
<p>Now, new scenario.
Imagine that you are a person (I know, I'm really making you work here).
You are reading some collection of words that are <strong>not news.</strong>
This could be a novel, a poem, or (importantly) a commentary written <em>about</em> something in the news (I hope we're all on the same page that these commentaries aren't news).
If I jumped out after you finished reading this and told you that it was written by an AI, what would you do?
My argument here is that your answer should be "I don't know and I don't really care. I was entertained."</p>
<p>What OpenAI seems to be saying with their lack of transparency over this whole thing is that this new language model is dangerous.
But I would ask the following: if this model is so good that we don't trust humans to read its output, has OpenAI just passed the Turing test?
Doubtful.
There simply is a model that does a better job of matching patterns in writing as determined by a very large corpus of writing.</p>
<p>So what should we do moving forward?</p>
<ul>
<li>OpenAI should release the model, or change their name</li>
<li>Humans should regain some confidence </li>
<li>You should change the locks in your house so I can't jump out from behind you anymore</li>
</ul>
<p>quick thoughts on the recent OpenAI kerfuffle:</p>
<ul>
<li>have more faith in humanity!!</li>
<li>if it's as bad as everyone thinks, have we sneakily passed some form of the Turing test? let's test it.</li>
<li>if the problem, is "we're worried that someone will ready a single random article on the internet and then say 'hmm, it's on abc123newsblog.com so it must be true!'" then we have much bigger problems as a society. </li>
</ul>
  </div><!-- .content -->

</section>


    <!-- footer -->
    <footer>
      <p>
        Â© John Lalor, license <a href=""> </a>
        unless otherwise noted.
        Generated by <a href= "http://docs.getpelican.com/">Pelican</a> with
        <a href="http://github.com/porterjamesj/crowsfoot">crowsfoot</a> theme.
      </p>
    </footer>
  </body>
</html>