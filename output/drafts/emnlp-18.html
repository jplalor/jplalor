<!DOCTYPE html>
<html lang="en">

  <head>
      <title>John Lalor</title>
      <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,700italic,400,600,700' rel='stylesheet' type='text/css' />
      <link href='http://fonts.googleapis.com/css?family=Merriweather:300' rel='stylesheet' type='text/css'/>
      <link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:200,400,700' rel='stylesheet' type='text/css'/>
      <link rel="stylesheet" type="text/css" href="../theme/css/icons.css"/>
      <link rel="stylesheet" type="text/css" href="../theme/css/styles.css"/>
      <meta charset="utf-8" />
  </head>

  <body id="index">
    <!-- header -->
    <header class="siteheader">
      <!-- site image -->
        <div class= "siteimage">
          <a class="nodec" href=images/me_head_2.png>
            <img width="200" height="200" src=images/me_head_2.png>
          </a>
        </div>

      <div class = "sitebanner">
        <h1><a class="sitetitle nodec" href="..">John Lalor</a></h1>
        <h3 class ="sitesubtitle"></h3>
        <!-- nav -->
        <nav class="menu">
          <ul>
            <!-- menu items-->
              <li><a class="nodec" href="/">home</a></li>
              <li><a class="nodec" href="/blog_index.html">blog</a></li>
              <li><a class="nodec" href="/pdfs/cv.pdf">cv</a></li>
            <!--pages-->
            <!-- services icons -->
              <li><a class="nodec icon-mail-alt" href="mailto:lalor@cs.umass.edu"></a></li>
              <li><a class="nodec icon-github" href="https://github.com/jplalor"></a></li>
          </ul>
        </nav>
      </div> <!-- sitebanner -->
    </header>

    <!-- content -->

<section class="content">

  <h3 class="posttitle">
    <a class="nodec" href="/drafts/emnlp-18.html" rel="bookmark" title="Permalink to Does Item Difficulty Affect DNN Performance?">
      Does Item Difficulty Affect DNN Performance?
    </a>
  </h3>

  <div class="postinfo">
    <p class="published" title="2019-02-21T00:00:00-05:00">
      Thu 21 February 2019
    </p>

  </div><!-- .postinfo -->

  <div class="article">
    <p>Short answer: yes.
The long answer is a bit more involved, hence the blog post.
This post is a companion to our recent EMNLP 2018 paper <a href="https://arxiv.org/abs/1702.04811">"Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study"</a>.
If you're really interested, I have a recorded version of the talk from EMNLP on <a href="https://www.youtube.com/watch?v=4FZYB-YvV7k">YouTube</a>.</p>
<h1>Introduction</h1>
<p>In <a href="http://jplalor.github.io/emnlp-16.html">an earlier post</a> I discussed how Item Response Theory (IRT) can be used to build more advanced test sets for natural language processing models. 
IRT tests include items for which we know certain latent parameters such as difficulty and discriminatory ability.
Our previous work looked at using these test sets to, well, test a deep neural network to see if reported high accuracy scores were really indicative of high latent ability.
Turns out that high accuracy on a very easy dataset doesn't directly translate to high ability, which makes sense.
If the test is easy then most everyone will do well, so your high score isn't as impressive.
On the other hand, a high (but maybe not very high) score on a difficulty test does indicate high ability, because you have done well on a test that most find difficult. 
Knowing item difficulty and latent ability lets us better understand performance, of humans and of neural networks.</p>
<p>The next question we wanted to answer was about the neural networks' performance on <em>specific</em> items.
We determined how scores change at the aggregate level, but can we learn anything about a model's performance based on the difficulty of specific items?
Do high-performing models do better on easy items than low-performing models, or do they do better on harder items, or both?</p>
<h1>The Goal</h1>
<p>What we want to do here is determine if the latent difficulty of an item is predictive of whether a neural network model will label the item correctly.
Put another way: are easy items more likely to be labeled correctly by a neural network than harder items? 
And if so, does this relationship change as our models get better at the task? 
We already have data with item difficulties, from our prior work.
So these items are now our test set for the models that we will use.
Every model will label the IRT test set, and we can use these outputs and the input characteristics of the models to learn a logistic regression model to predict whether a model will label an item correctly, given some input set of features. </p>
<h1>Different Data, Different model</h1>
<p>How can we get a model that performs better or worse while maintaining the underlying structure so we can make meaningful comparisons?
One straightforward approach is to modify the training data, so that the same model can be trained with a variety of different training sets.
For a typical NLP task, there is some large data set that has been gathered and released to the research community.
It usually consists of a split between training, testing, and (sometimes) validation sets so that results can be compared between research groups.
When you build a new model, you use the entire training set to train your model, because more data is better and this will show the best possible performance for your model.
However, in our case we want a variety of performance levels, so to do that we can sample from the training set, and use this training subset to train several different models to label our IRT items. </p>
<h1>Predicting Correctness</h1>
<p>We know the output we want to predict: whether a given model labeled an item correctly or not.
What are the inputs to our model?
We kept it simple and used two input features: </p>
<ol>
<li>The difficulty of the specific item </li>
<li>The training set size used to train the model</li>
</ol>
<p>These two features cover the two questions we are trying to answer:</p>
<ol>
<li>Does item difficulty affect predictive performance?</li>
<li>How important is training size on predictive performance?</li>
</ol>
<p>If we also model the interaction between these features, we can see if there are interesting changes as one or both features vary as well. </p>
  </div><!-- .content -->

</section>


    <!-- footer -->
    <footer>
      <p>
        Â© John Lalor, license <a href=""> </a>
        unless otherwise noted.
        Generated by <a href= "http://docs.getpelican.com/">Pelican</a> with
        <a href="http://github.com/porterjamesj/crowsfoot">crowsfoot</a> theme.
      </p>
    </footer>
  </body>
</html>