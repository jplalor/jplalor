<!DOCTYPE html>
<html lang="en"><head>
	
	<meta name="generator" content="Hugo 0.75.1" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="author" content="John P. Lalor"><meta name="description" content=""><meta property="og:title" content="Item Response Theory for Natural Language Processing" />
<meta property="og:description" content="Notes on our EMNLP16 paper" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/emnlp16/" />
<meta property="article:published_time" content="2019-02-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-02-12T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Item Response Theory for Natural Language Processing"/>
<meta name="twitter:description" content="Notes on our EMNLP16 paper"/>
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
	<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
	<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/all.css" />
	<link rel="stylesheet" href="/css/katex.min.css" crossorigin="anonymous">
	<script defer src="/js/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
	<script defer src="/js/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><title>Item Response Theory for Natural Language Processing | John P. Lalor</title></head>
<body><header>
	
	<div id="avatar">
		<a href="/">
		  <img src="/img/headshot_cropped.jpg" alt="John P. Lalor">
		</a>
	</div>
	
	<div id="titletext"><h2 id="title"><a href="/">John P. Lalor</a></h2></div>
	<div id="title-description"><p id="subtitle">Assistant Professor of IT, Analytics, and Operations</p><div id="social">
			<nav>
				<ul>
					<li><a href="https://github.com/jplalor"><i title="Github" class="icons fab fa-github"></i></a></li><li><a href="mailto:john.lalor@nd.edu"><i title="Email" class="icons fas fa-envelope"></i></a></li></ul>
			</nav>
		</div>
	</div>
	
	<div id="mainmenu">
		<nav>
			<ul>
				
				<li><a href="/">Home</a></li>
				
				<li><a href="/research">Research</a></li>
				
				<li><a href="/teaching">Teaching</a></li>
				
				<li><a href="/post">Blog</a></li>
				
				<li><a href="/pdf/cv.pdf">CV</a></li>
				
			</ul>
		</nav>
	</div>
	
</header>
<main><div class="post">
	
	<div class="author">
	
	</div>
	<div class="post-header">
	
		<div class="meta">
			
			<div class="date">
				<span class="day">12</span>
				<span class="rest">Feb 2019</span>
			</div>
			
		</div>
		
		<div class="matter">
			<h1 class="title">Item Response Theory for Natural Language Processing</h1>
		</div>
	</div>
	<div class="markdown">
		


<p>This post is meant as a companion to our EMNLP 2016 paper <a href="https://arxiv.org/abs/1605.08889">“Building an Evaluation Scale using Item Response Theory”</a>.
It’s quite a bit overdue, but hopefully this post will be useful to those who haven’t seen IRT before.</p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Let’s start by thinking about the typical supervised machine learning setup.
There is some training data, a held-out test set, and a machine learning model.
The goal is to use the training data to learn a model that performs well on the test set.
“Performs well” is usually measured by some aggregate statistic such as accuracy, precision/recall, etc.
These aggregate statistics assume that each test set example is as important in determining model performance as every other test set example.
But what if that isn’t the case?
What if certain examples are so easy that labelling them incorrectly is disastrous?
Or on the other hand, what if certain examples are so hard that no model labels them correctly (except your new deep deep deep network)?</p>
<p>Characteristics such as difficulty are often used to assess humans in psychometrics, specifically a method known as Item Response Theory (IRT).
The high-level idea with IRT is that if you have enough test-takers providing answers to questions on a test (“items,” hence the Item in IRT), you can learn latent parameters of the items as well as estimate a latent ability trait of the test-takers themselves.
IRT is popular in standardized tests such as the SAT and the GMAT.
It’s used to assess the test-takers but also to select appropriate items for the tests themselves (if a test question is too easy, there’s no need to include it).
What we wanted to do was take the IRT methodology and apply it to machine learning models, specifically models trained to do the natural language processing (NLP) natural language inference (NLI) task.</p>
</div>
<div id="irt" class="section level1">
<h1>IRT</h1>
<p>The key driver behind IRT is what’s known as the Item Characteristic Curve (ICC).
Each item has an associated ICC, which can take certain forms depending on the IRT model that you’re looking at.
A popular model (and the one we used in our paper) is the 3 Parameter Logistic (3PL) model:</p>
<p><span class="math display">\[
p_{ij}(\theta_j) = c_i + \frac{1 - c_i}{1 + e^{-a_i(\theta_j - b_i)}}
\]</span>
Here, <span class="math inline">\(\theta_j\)</span> is the latent ability parameter of test-taker <span class="math inline">\(j\)</span>, and <span class="math inline">\(a_i\)</span>, <span class="math inline">\(b_i\)</span>, and <span class="math inline">\(c_i\)</span> are item <span class="math inline">\(i\)</span>’s discriminatory, difficulty, and guessing parameters, respectively.
A typical IRT curve will look something like this:</p>
<p><img src="/post/emnlp16_files/figure-html/irtplot1-1.png" width="672" /></p>
<p>Our x-axis is <span class="math inline">\(\theta_j\)</span>, the latent ability parameter.
The y-axis is <span class="math inline">\(p_{ij}(\theta_j)\)</span>, the probability that an individual with a certain ability level will answer this item correctly.
The curve is monotonically increasing, which makes sense.
As the ability of an individual increases, we expect that the probability of that individual answering correctly will also increase.
<span class="math inline">\(a_i\)</span>, the discriminatory parameter, represents the slope of the curve at its steepest point.
<span class="math inline">\(a_i\)</span> should be steep enough that in a relatively short range, there is a sizeable jump in <span class="math inline">\(p_{ij}(\theta_j)\)</span>, but shouldn’t be so steep that the range is tiny.
An item with too steep of a slope is only useful in a very small ability range:</p>
<p><img src="/post/emnlp16_files/figure-html/irtplot2-1.png" width="672" /></p>
<p><span class="math inline">\(b_i\)</span>, the difficulty parameter, represents the halfway point between the minimum and maximum values of <span class="math inline">\(p_{ij}(\theta_j)\)</span>.
For a 3PL model, the minimum is <span class="math inline">\(c_i\)</span> and the maximum is <span class="math inline">\(1\)</span>.
Since <span class="math inline">\(\theta\)</span> is a unit Gaussian (<span class="math inline">\(\theta_j \sim N(0, 1)\)</span>), we want the value of <span class="math inline">\(b_i\)</span> to fall somewhere between <span class="math inline">\(-3\)</span> and <span class="math inline">\(3\)</span> in most cases, since that covers 99.7% of people.
The next two plots show “easy” and “hard” items where the difficulty parameters are <span class="math inline">\(-2\)</span> and <span class="math inline">\(2\)</span>, respectively:</p>
<p><img src="/post/emnlp16_files/figure-html/irtplot3-1.png" width="672" /></p>
<p><img src="/post/emnlp16_files/figure-html/irtplot4-1.png" width="672" /></p>
<p><span class="math inline">\(c_i\)</span> is referred to as the guessing parameter. As you can see from the plots above, the lower asymptotes of the curves are not <span class="math inline">\(0\)</span>. In a three parameter model, there is an assumption that even at low levels of <span class="math inline">\(\theta\)</span>, there is some non-zero probability that an individual will answer a question correctly (with for example a lucky guess). That is modeled by <span class="math inline">\(c_i\)</span>.</p>
</div>
<div id="irt-for-nlp" class="section level1">
<h1>IRT for NLP</h1>
<p>So how can we use IRT to help us in NLP?
It’s often the case that when you train and test your brand new machine learning model on an NLP dataset, the metric of interest is test set accuracy.
If your model has a higher accuracy than the current state of the art in the literature, then your model becomes the new benchmark against which other models are evaluated.
However, there is usually something missing in these evaluations: the question of the specific test set examples that were answered correctly.
If your fancy new model has a high accuracy because it labeled a random sequence of examples correctly, that indicates very different performance than some other model that labeled items correctly according to their difficulty (i.e. labeled all easy examples correctly up to some difficulty threshold, then labeled all subsequent examples incorrectly).
Having some notion of difficulty is important to distinguish between these two cases, which is where IRT comes in!</p>
<p>We decided to test this out on the Stanford Natural Language Inference (SNLI) dataset. Natural language inference (NLI) is a popular task in NLP, where the goal is to determine if some sentence (premise) entails some other sentence (hypothesis). If the premise is true, does that mean that the hypothesis must be true (entailment), cannot be true (contradiction), or could be either (neutral)? You can imagine that there might be some ambiguity about certain examples having certain labels. So this seems like a perfect place to apply IRT to see if we can learn anything about the data and the high-performing models trained on the dataset.</p>
</div>
<div id="data-collection" class="section level1">
<h1>Data Collection</h1>
<p>In order to fit these IRT models, you need a lot of data. Specifically, you need a lot of answers to the same set of questions. If you ask a lot of people to take the same test, then grade each of their answers, the graded responses for each person is that person’s response pattern. With a set of response patterns you can fit an IRT model to learn the latent parameters of the items as well as the latent <span class="math inline">\(\theta\)</span> for each of the test-takers.</p>
<p>This goes against what is typically done when you are gathering data for machine learning models. Usually, you will collect between one and five labels for each example in your data set, and use a majority vote to determine the gold standard. Here, we used the crowdsourcing platform Amazon Mechanical Turk to gather 1000 labels for each example from our SNLI subset. So instead of asking a lot of Turkers for a few labels to a lot of examples, we asked them for a lot of labels for a small number of examples. Each Turker provided a label for each example, so we were able to grade the Turker responses to generate response patterns.</p>
</div>
<div id="irt-analysis" class="section level1">
<h1>IRT Analysis</h1>
<p>Fitting an IRT model is a cyclical process. At a high level, you fit your model, using your software package of choice (for R, the <em>mirt</em> package is a good one). Then you check how well each item fits in with the learned model. Are there items where the discriminatory parameter is too large? Items like these aren’t useful outside of a very specific range so they can be removed. Items where the parameter is too small are also removed, because they are not able to discriminate at all. Are there items that violate the local independence assumption? These are also removed.
Typically the items are removed one at a time, the model is re-fit, and the set of items is checked again.</p>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<p>There are a number of results in the paper, but to boil it down into a single takeaway: specific data points are important! If you are testing your model on an easy dataset, then high accuracy scores aren’t all that impressive, because everyone has a high accuracy. On the other hand, if you test on a difficult dataset, then lower accuracy scores may be indicative of higher latent <span class="math inline">\(\theta\)</span>, since the score is with respect to some population of test-takers.</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Hopefully this was a useful introduction to IRT. I encourage you to check out the paper for much more detail. And feel free to email me if you have any questions or comments. The next post will discuss our EMNLP 2018 paper: Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study.</p>
</div>

	</div>
	
	
	
	
	
		
	
		
	
		
		
	</div></div>

  </main>
<footer>
	 © Copyright John P. Lalor | <a href="https://github.com/dataCobra/hugo-vitae">Vitae</a> theme for <a href="https://gohugo.io">Hugo</a> 
	
	
	
</footer>


</body>
</html>
