<!DOCTYPE html>
<html lang="en"><head>
	
	<meta name="generator" content="Hugo 0.78.2" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="author" content="John P. Lalor"><meta name="description" content=""><meta property="og:title" content="Essential Information Theory, Part 1" />
<meta property="og:description" content="Entropy!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jplalor.github.io/post/info-theory-part-1/" />
<meta property="article:published_time" content="2018-02-15T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-02-15T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Essential Information Theory, Part 1"/>
<meta name="twitter:description" content="Entropy!"/>
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
	<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
	<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/all.css" />
	<link rel="stylesheet" href="/css/katex.min.css" crossorigin="anonymous">
	<script defer src="/js/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
	<script defer src="/js/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><title>Essential Information Theory, Part 1 | John P. Lalor</title></head>
<body><header>
	
	<div id="avatar">
		<a href="https://jplalor.github.io/">
		  <img src="/img/headshot_cropped.jpg" alt="John P. Lalor">
		</a>
	</div>
	
	<div id="titletext"><h2 id="title"><a href="https://jplalor.github.io/">John P. Lalor</a></h2></div>
	<div id="title-description"><p id="subtitle">Assistant Professor of IT, Analytics, and Operations</p><div id="social">
			<nav>
				<ul>
					<li><a href="https://github.com/jplalor"><i title="Github" class="icons fab fa-github"></i></a></li><li><a href="mailto:john.lalor@nd.edu"><i title="Email" class="icons fas fa-envelope"></i></a></li></ul>
			</nav>
		</div>
	</div>
	
	<div id="mainmenu">
		<nav>
			<ul>
				
				<li><a href="/">Home</a></li>
				
				<li><a href="/research">Research</a></li>
				
				<li><a href="/teaching">Teaching</a></li>
				
				<li><a href="/post">Blog</a></li>
				
				<li><a href="/pdf/cv.pdf">CV</a></li>
				
			</ul>
		</nav>
	</div>
	
</header>
<main><div class="post">
	
	<div class="author">
	
	</div>
	<div class="post-header">
	
		<div class="meta">
			
			<div class="date">
				<span class="day">15</span>
				<span class="rest">Feb 2018</span>
			</div>
			
		</div>
		
		<div class="matter">
			<h1 class="title">Essential Information Theory, Part 1</h1>
		</div>
	</div>
	<div class="markdown">
		


<p>I’d like to blog regularly, but I often struggle to come up with topics. So what I’m going to do is blog my way through some textbooks, to work on my writing and also to make sure that I have the concepts in the books down pat. The first textbook in the series will be <em>Foundations of Statistical Natural Language Processing</em> by Manning and Schütze (which I’ll refer to as StatNLP from here on out). And the first topic will be <em>Essential Information Theory</em> (section 2.2 in the book). This first post will introduce entropy and provide a few examples.</p>
<div id="information-theory-basics" class="section level1">
<h1>Information Theory Basics</h1>
<p>Information Theory as a field was developed by Claude Shannon in the 1940s. I won’t go into too many details, but basically Shannon was working on a way to quantify how much information one could transmit through a channel that would inevitably lose some of the information due to some amount of noise. He introduced some key concepts that we’ll define and visualize here. Information Theory as a whole is an active field, and we’ll only scratch the surface here, but at the end of the series I’ll provide some pointers for anyone interested in learning more.</p>
<div id="preliminaries" class="section level2">
<h2>Preliminaries</h2>
<p>In this post we’ll be looking at probabilities of random variables. We’ll define a discrete random variable as a set of outcomes <span class="math inline">\(X\)</span>. Each potential outcome in <span class="math inline">\(X\)</span> has an associated probability: <span class="math inline">\(p(X=x)\)</span>. The <em>expectation</em> (or <em>expected value</em>) of <span class="math inline">\(X\)</span> is a weighted sum of the outcomes according to the probabilities:</p>
<p><span class="math display">\[\mathbb{E}[X] = \sum_{x \in X} xp(x)\]</span></p>
</div>
<div id="entropy" class="section level2">
<h2>Entropy</h2>
<p>One of the key pieces in Information Theory is entropy. Entropy is defined as the amount of <em>information</em> in a random variable. The formula for entropy is</p>
<p><span class="math display">\[H(X) = - \sum_{x \in X} p(x) \log p(x)\]</span></p>
<p>Often times the logarithm in the formula is base 2, and the output is a measure of <em>bits</em>. Sometimes you’ll see the natural log (<span class="math inline">\(\ln\)</span>), and the output will be in <em>nats</em>. You can also write this formula a few other ways. If you don’t like the minus sign in the beginning there, you can move it in and use the reciprocal of the logarithm:</p>
<p><span class="math display">\[H(X) = \sum_{x \in X} p(x) \log \frac{1}{p(x)}\]</span>
<span class="math inline">\(\log \frac{1}{p(x)}\)</span> is known as the Shannon information content of an outcome (or information content for short). It tells you how much information you get for a specific outcome of a random variable. The previous formula looks a lot like an expectation. That’s because it is! So we can rewrite the formula for entropy again as:</p>
<p><span class="math display">\[H(X) = \mathbb{E}( \log \frac{1}{p(X)})\]</span></p>
<p>Notice in this last formula that the <span class="math inline">\(p(x)\)</span> is now <span class="math inline">\(p(X)\)</span> (capital <span class="math inline">\(X\)</span> instead of lowercase <span class="math inline">\(x\)</span>). When we have a lowercase <span class="math inline">\(x\)</span>, we’re looking for the probability of a specific value <span class="math inline">\(x\)</span> which is in the set of possible values <span class="math inline">\(X\)</span>. In the third equation, we’re calculating the expectation of the random variable <span class="math inline">\(X\)</span>, and so we use <span class="math inline">\(p(X)\)</span>.</p>
<p>So what do all of these equivalent formulas mean? When we’re calculating entropy, what we’re really calculating is the amount of information contained in a particular random variable. On one extreme, if we know everything about the random variable (if <span class="math inline">\(p(x) = 1\)</span> for one of the potential outcomes), then there is no information to be gained from the random variable. On the other extreme, if all of the outcomes are equally likely (as in a standard six-sided die) then there is a lot of information in the random variable.</p>
<p>I always like to think about entropy in terms of flipping a coin. Let’s say we have a completely unbiased coin, so that the probability of it landing on heads is equal to the probability of it landing on tails. Our random variable is still <span class="math inline">\(X\)</span>, and the possible values are <span class="math inline">\(x = H\)</span> and <span class="math inline">\(x = T\)</span>. The probability of each outcome is <span class="math inline">\(p(x = H) = p(x = T) = 0.5\)</span>.</p>
<p>Let’s plug this in to our formula for entropy (we’ll use the 2nd formula above).</p>
<p><span class="math display">\[
H(X) = \sum_{x \in \{H, T\}}p(x) \log \frac{1}{p(x)} \\
 = 0.5 * \log \frac{1}{0.5} + 0.5 * \log \frac{1}{0.5} \\
 = 2*(0.5 \log 2) \\
 = 1
\]</span></p>
<p>So what this tells us is that every time we flip an unbiased coin, we get 1 bit of information. what if the coin was biased? For example, let’s say that the probability of heads is <span class="math inline">\(0.75\)</span> and the probability of tails is <span class="math inline">\(0.25\)</span>. In that case when we calculate entropy we get</p>
<p><span class="math display">\[
H(X) = \sum_{x \in \{H, T\}}p(x) \log \frac{1}{p(x)} \\
 = 0.75 * \log \frac{1}{0.75} + 0.25 * \log \frac{1}{0.25} \\
 \approx 0.81
\]</span>
We get less information if we flip a biased coin. Why is that? Because there is less uncertainty in the outcome of the coin flip. If the coin is biased, then one side is more likely to come up than the other. In the case above, if the probability of getting heads is <span class="math inline">\(0.75\)</span>, then before you even flip it the odds are good that it will come up heads. So you don’t get as much information as when the coin is unbiased, where there is more uncertainty (in fact, the maximum amount of uncertainty).</p>
<p>Let’s look at a plot of entropy as the bias of a coin changes.</p>
<p><img src="/post/info_theory_files/figure-html/entropy_plot-1.png" width="672" /></p>
<p>As the plot shows, the more biased the coin is, the less information we gain when we flip it. Sometimes it helps to think about entropy in terms of being surprised. When we flip a coin biased to land on heads, we’re less surprised when it does land on heads (lower entropy). But when a coin is unbiased, each flip is a surprise (high entropy).</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Entropy is a key building block for Information Theory, and it is also very useful in Machine Learning and NLP (more in a later post). Next time we’ll look at entropy for more than one random variable, and define joint entropy, conditional entropy, and (probably) KL-Divergence.</p>
<p>As a last note, I wrote this post in RMarkdown, which was surprisingly easy to integrate with this site, which runs on Pelican. All I needed was the <a href="https://github.com/getpelican/pelican-plugins/tree/master/rmd_reader">RMD Reader Pelican Plugin</a>.
(<em>Update 09/2020: this site is no longer using Pelican, I’ve moved to blogdown/Hugo</em>)</p>
</div>

	</div>
	
	
	
	
	
		
	
		
	
		
		
	</div></div>

  </main>
<footer>
	 © Copyright John P. Lalor | <a href="https://github.com/dataCobra/hugo-vitae">Vitae</a> theme for <a href="https://gohugo.io">Hugo</a> 
	
	
	
</footer>


</body>
</html>
