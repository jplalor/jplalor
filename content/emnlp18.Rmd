---
title: Does Item Difficulty Affect DNN Performance?
author: John Lalor
date: February 21, 2019
output: html
Category: IRT
Slug: emnlp-18
Summary: Notes on our EMNLP18 paper
Tags: ml, nlp, irt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(latex2exp)
```

Short answer: yes.
The long answer is a bit more involved, hence the blog post.
This post is a companion to our recent EMNLP 2018 paper ["Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study"](https://arxiv.org/abs/1702.04811).
If you're really interested, I have a recorded version of the talk from EMNLP on [YouTube](https://www.youtube.com/watch?v=4FZYB-YvV7k).

# Introduction

In [an earlier post](http://jplalor.github.io/emnlp-16.html) I discussed how Item Response Theory (IRT) can be used to build more advanced test sets for natural language processing models. 
IRT tests include items for which we know certain latent parameters such as difficulty and discriminatory ability.
Our previous work looked at using these test sets to, well, test a deep neural network to see if reported high accuracy scores were really indicative of high latent ability.
Turns out that high accuracy on a very easy dataset doesn't directly translate to high ability, which makes sense.
If the test is easy then most everyone will do well, so your high score isn't as impressive.
On the other hand, a high (but maybe not very high) score on a difficulty test does indicate high ability, because you have done well on a test that most find difficult. 
Knowing item difficulty and latent ability lets us better understand performance, of humans and of neural networks.

The next question we wanted to answer was about the neural networks' performance on *specific* items.
We determined how scores change at the aggregate level, but can we learn anything about a model's performance based on the difficulty of specific items?
Do high-performing models do better on easy items than low-performing models, or do they do better on harder items, or both?

# The Goal

What we want to do here is determine if the latent difficulty of an item is predictive of whether a neural network model will label the item correctly.
Put another way: are easy items more likely to be labeled correctly by a neural network than harder items? 
And if so, does this relationship change as our models get better at the task? 
We already have data with item difficulties, from our prior work.
So these items are now our test set for the models that we will use.
Every model will label the IRT test set, and we can use these outputs and the input characteristics of the models to learn a logistic regression model to predict whether a model will label an item correctly, given some input set of features. 

# Different Data, Different model

How can we get a model that performs better or worse while maintaining the underlying structure so we can make meaningful comparisons?
One straightforward approach is to modify the training data, so that the same model can be trained with a variety of different training sets.
For a typical NLP task, there is some large data set that has been gathered and released to the research community.
It usually consists of a split between training, testing, and (sometimes) validation sets so that results can be compared between research groups.
When you build a new model, you use the entire training set to train your model, because more data is better and this will show the best possible performance for your model.
However, in our case we want a variety of performance levels, so to do that we can sample from the training set, and use this training subset to train several different models to label our IRT items. 




