Title: Evaluation Scales with Item Response Theory
Date: 2016-10-11 11:03
Category: static
Author: John Lalor
Summary: Code and data for our EMNLP 2016 paper
save_as: irt.html

## Overview

This page contains code and data for our IRT analyses.

## SNLI

If you use the following data, please cite:

- J.P. Lalor, H. Wu, H. Yu, **Building an Evaluation Scale using Item Response Theory**, In *EMNLP 2016*. [arXiv pre-print][1]

### Data

The dataset consists of response patterns collected using the Amazon Mechanical Turk crowdsourcing platform.

Included in the zip file is the data and a README.

License: The data is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License][3].
Based on the [Stanford SNLI project][4].

Download: [zip file][2]

### Code

Code used to generate the evaluation scales from the paper was written in R.
Included are R files for each of the 5 evaluation scales.

Download: [code hosted on GitHub][5]


## Sentiment Analysis

If you use the following data please cite:

- J.P. Lalor, H. Wu, T. Munkhdalai, H. Yu. Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study. In *EMNLP 2018.* [arxiv pre-print][6]

### Data

[download data][7]

Questions about the code or data? Contact me at lalor at cs dot umass dot edu.

[1]:https://arxiv.org/abs/1605.08889v2
[2]:/files/data_emnlp2016.zip
[3]:http://creativecommons.org/licenses/by-sa/4.0/
[4]:http://http://nlp.stanford.edu/projects/snli/
[5]:https://github.com/jplalor/irt-models
[6]:https://arxiv.org/abs/1702.04811
[7]:/files/emnlp2018.tar.gz