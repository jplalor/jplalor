---
title: StatNLP: Giving NLP Models Standardized Tests
author: John Lalor
date: October 17, 2018
output: html
Category: IRT
Slug: emnlp-16
Summary: Notes on our EMNLP16 paper
Tags: ml, nlp, irt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This post is meant as a companion to our EMNLP 2016 paper ``Building an Evaluation Scale using Item Response Theory'' [[arXiv link][https://arxiv.org/abs/1605.08889]]. 
It's quite a bit overdue, but hopefully this post will be useful to those who haven't seen IRT before.

# Introduction

Let's start by thinking about the typical supervised machine learning setup. 
There is some training data, a held-out test set, and a machine learning model.
The goal is to use the training data to learn a model that performs well on the test set.
``Performs well'' is usually measured by some aggregate statistic such as accuracy, precision/recall, etc.
These aggregate statistics assume that each test set example is as important in determining model performance as every other test set example.
But what if that isn't the case?
What if certain examples are so easy that labelling them incorrectly is disastrous?
Or on the other hand, what if certain examples are so hard that no model labels them correctly (except your new deep deep deep network)?

Characteristics such as difficulty are often used to assess humans in psychometrics, specifically a paradigm known as Item Response Theory (IRT).
The high-level idea with IRT is that if you have enough test-takers provide answers to questions on a test (``items,'' hence the Item in IRT), you can learn latent parameters of the items as well as estimate a latent ability trait of the test-takers themselves.
IRT is popular in standardized tests such as the SAT and the GMAT.
It's used to assess the test-takers but also to select appropriate items for the tests themselves (if a test question is too easy, there's no need to include it).
What we wanted to do was take the IRT methodology and apply it to machine learning models, specifically models trained to do the natural language processing (NLP) natural language inference (NLI) task.

# IRT 

The key driver behind IRT is what's known as the Item Characteristic Curve (ICC).
Each item has an associated ICC, which can take certain forms depending on the IRT model that you're looking at.
A popular model (and the one we used in our paper) is the 3 Parameter Logistic (3PL) model:

$$
p_{ij}(\theta_j) = c_i + \frac{1 - c_i}{1 + e^{-a_i(\theta_j - b_i)}}
$$
Here, $\theta_j$ is the latent ability parameter of test-taker $j$, and $a_i$, $b_i$, and $c_i$ are item $i$'s discriminatory, difficulty, and guessing parameters, respectively.
 A typical IRT curve will look something like this:
 
```{r entropy_plot, echo=FALSE}

icc_curve <- function(a, b, c, theta){
  return(c + ((1 - c)/(1 + exp(-a * (theta - b)))))
}
x <- seq(-4, 4, by=0.1)
y <- mapply(icc_curve, 1, 0, 0.25, x)
plot(x, y, ylim = c(0, 1), type='l', xlab = "$theta$", ylab="p($theta$)")
```



$$
\begin{align}
H(X, Y) &= - \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y) \\
&= \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{p(x, y)}
\end{align}
$$

Joint entropy refers to the amount of information you would need to determine the values of two random variables.
As an example, we'll go back to coin flipping from last time.
Now, we have two independent coins, both of them unbiased (so $p(x=H) = p(x=T) = p(y=H) = p(y=T) = 0.5$).

The joint entropy for these two random variables is as follows (below the order for the variables is $(x,y)$, so $p(H,H)$ is really $p(x=H, y=H)$:

$$
\begin{align}
H(X, Y) &=  \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{ p(x, y)} \\
&= p(H,H)\log \frac{1}{p(H,H)} + p(H,T)\log \frac{1}{p(H,T)} \\
&+ p(T,H)\log \frac{1}{p(T,H)} + p(T,T)\log \frac{1}{p(T,T)} \\
&= \frac{1}{4}\log 4 + \frac{1}{4}\log 4+ \frac{1}{4}\log 4+ \frac{1}{4}\log 4 \\
&= 2
\end{align}
$$
This should make sense, since we showed last time that the entropy of a single unbiased coin is 1 bit. If we have two coins, we then double our information.

## Conditional Entropy
Conditional entropy is related to joint entropy. We're still dealing with two random variables, but with conditional entropy we're interested in how much information is gained *when we know one of the outcomes.* 

$$
\begin{align}
H(Y \vert X) &= \sum_{x \in X} p(x) H(Y \vert X = x) \\
&= \sum_{x \in X} p(x) [\sum_{y \in Y} p(y \vert x) \log \frac{1}{p(y \vert x)}] \\
&= \sum_{x \in X} \sum_{y \in Y} p(x, y) log \frac{1}{p(y \vert x)}
\end{align}
$$

Ok, so what just happened? Let's take the previous set of equations one at a time. In the first line we define the conditional entropy as a weighted average of the conditional entropies of $Y$ given each possible value of $X$. In the second line we just expand that out, using the definition of entropy. Finally, $p(x)p(y|x) = p(x,y)$ so we can rewrite the equation as in the third line.

Joint and conditional entropy are closely related. With joint entropy, we are measuring the amount of information in two random variables. Conditional entropy we already know one of the two random variables, and so we're measuring how much additional information is in the other. We can also look at it mathematically, starting with the definition of joint entropy:

$$
\begin{align}
H(X, Y) &=  \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{ p(x, y)} \\
&= \sum_{x \in X} \sum_{y \in Y} p(x,y) \log \frac{1}{ p(x) p(y\vert x)} \\
&= \sum_{x \in X} \sum_{y \in Y} p(x,y)[ \log \frac{1}{p(x)} + \log \frac{1}{p(y \vert x)}] \\
&= \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{p(x)} + \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{p(y \vert x)} \\
&= \sum_{x \in X} p(x) \log \frac{1}{p(x)} + \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{p(y \vert x)} \\
&= H(X) + H(Y \vert X)
\end{align}
$$

Let's walk through that. In eqn. 12, we rewrite $p(x,y)$ in the log. In eqn 13. we split that log product into a sum of logs. In eqn. 14 we move $p(x,y)$ into the sum, and in eqn. 15 we use the fact that $\sum_{y \in Y} p(x, y) = p(x)$.

So to get the joint entropy of two random variables, you calculate the entropy of one, and add the conditional entropy of the other, given the first one. If the two random variables are independent, then $H(X, Y) = H(X) + H(Y)$. We saw that above with our two coin example. If you swap all of the x's and y's in the above, you'll see that the order doesn't matter, and $H(X,Y) = H(X) + H(Y \vert X) = H(Y) + H(X \vert Y)$


## Wrapping Up

There were a lot of equations here, but hopefully they all make sense. We're still trying to measure information (as in standard entropy for a single random variable) but now we can look at the information relationship between pairs of random variables. Next time we'll look at two more advanced concepts: mutual information and KL-Divergence.
