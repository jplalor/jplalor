---
title: StatNLP: Essential Information Theory, Part 2
author: John Lalor
date: February 23, 2018
output: html
Category: StatNLP
Slug: info-theory-part-2
Summary: Joint and Conditional Entropy
Tags: ml, nlp
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Welcome back! In the last post we started our discussion of Information Theory, and defined some key building blocks related to calculating information for a single random variable. This time we'll look at the relationships between two random variables, and what they can tell us (if anything) about the informativeness of each other.

## Joint Entropy

The first thing we'll look at is **joint entropy**. For two random variables, $X$ and $Y$, the joint entropy between them is 

$$
\begin{align}
H(X, Y) &= - \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y) \\
&= \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{p(x, y)}
\end{align}
$$

Joint entropy refers to the amount of information you would need to determine the values of two random variables.
As an example, we'll go back to coin flipping from last time.
Now, we have two independent coins, both of them unbiased (so $p(x=H) = p(x=T) = p(y=H) = p(y=T) = 0.5$).

The joint entropy for these two random variables is as follows (below the order for the variables is $(x,y)$, so $p(H,H)$ is really $p(x=H, y=H)$:

$$
\begin{align}
H(X, Y) &=  \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{1}{ p(x, y)} \\
&= p(H,H)\log \frac{1}{p(H,H)} + p(H,T)\log \frac{1}{p(H,T)} \\
&+ p(T,H)\log \frac{1}{p(T,H)} + p(T,T)\log \frac{1}{p(T,T)} \\
&= \frac{1}{4}\log 4 + \frac{1}{4}\log 4+ \frac{1}{4}\log 4+ \frac{1}{4}\log 4 \\
&= 2
\end{align}
$$
This should make sense, since we showed last time that the entropy of a single unbiased coin is 1 bit. If we have two coins, we then double our information.

## Conditional Entropy
Conditional entropy is related to joint entropy. We're still dealing with two random variables, but with conditional entropy we're interested in how much information is gained *when we know one of the outcomes.* 

$$
\begin{align}
H(Y \vert X) &= \sum_{x \in X} p(x) H(Y \vert X = x) \\
&= \sum_{x \in X} p(x) [\sum_{y \in Y} p(y \vert x) \log \frac{1}{p(y \vert x)}] \\
&= \sum_{x \in X} \sum_{y \in Y} p(x, y) log \frac{1}{p(y \vert x)}
\end{align}
$$

Ok, so what just happened? Let's take the previous set of equations one at a time. In the first line we define the conditional entropy as a weighted average of the conditional entropies of $Y$ given each possible value of $X$. In the second line we just expand that out, using the definition of entropy. Finally, $p(x)p(y|x) = p(x,y)$ so we can rewrite the equation as in the third line.

*Relationship between conditional and joint*

*chain rule*

*Plots*



## Wrapping Up
